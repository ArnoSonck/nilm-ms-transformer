{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "580370fc",
   "metadata": {},
   "source": [
    "\n",
    "# NILM — Transformer con Atención Multiescala (Demo Didáctica)\n",
    "\n",
    "Este cuaderno muestra un **modelo secuencia a secuencia** para **NILM** que emplea un **Transformer con atención multiescala**: una atención a escala **fina** (paso a paso) y otra a escala **gruesa** (después de *downsample*), cuyas salidas se **fusionan** para mejorar la captura de **transitorios rápidos** y **patrones de largo plazo**.\n",
    "\n",
    "> **Objetivo**: Dado un segmento de la **señal agregada** (potencia total), predecir la **potencia del aparato objetivo** para cada instante.\n",
    "\n",
    "**Contenido**:\n",
    "1. Instalación rápida de dependencias (si aún no están instaladas).\n",
    "2. Definición del modelo (PositionalEncoding, atención multiescala, Transformer).\n",
    "3. Dataset sintético (para que puedas ejecutar sin archivos externos).\n",
    "4. Entrenamiento mínimo y validación.\n",
    "5. Visualización de predicciones.\n",
    "6. Puntos de anclaje para usar datasets reales (UK-DALE/REFIT).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5e66e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== 1) Instalación rápida (opcional) ====\n",
    "# Si ejecutas en un entorno limpio, descomenta estas líneas:\n",
    "# %pip install torch --quiet\n",
    "# %pip install matplotlib numpy scikit-learn --quiet\n",
    "# (Si usas GPU con CUDA, revisa instrucciones específicas de PyTorch en https://pytorch.org/get-started/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d960b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== 2) Imports ====\n",
    "import math, time, os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af616c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== 3) Positional Encoding ====\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Agrega información de posición (tiempo) a los embeddings.\n",
    "    Implementación clásica (seno/coseno).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, max_len: int = 10000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, T, d_model]\n",
    "        \"\"\"\n",
    "        T = x.size(1)\n",
    "        return x + self.pe[:T, :].unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbceccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== 4) Atención estándar (una escala) ====\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"Self-Attention estándar de Transformer.\"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model debe ser múltiplo de num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads\n",
    "\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, D = x.shape\n",
    "        qkv = self.qkv(x)                    # [B, T, 3D]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)       # cada uno [B, T, D]\n",
    "\n",
    "        def split_heads(t):\n",
    "            return t.view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        q, k, v = map(split_heads, (q, k, v))\n",
    "\n",
    "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.d_head)  # [B, h, T, T]\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        attn = scores.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = attn @ v  # [B, h, T, d_head]\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, D)  # [B, T, D]\n",
    "        out = self.proj(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b792eddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== 5) Bloque Multiescala ====\n",
    "class MultiScaleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Atención a dos escalas:\n",
    "      - fina: T pasos\n",
    "      - gruesa: T/s pasos (después de AvgPool1d con factor s)\n",
    "    Fusión: concat([fino, grueso_up]) -> proyección -> residual + MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=128, num_heads=4, mlp_ratio=4, dropout=0.1, scale_factor=4):\n",
    "        super().__init__()\n",
    "        self.scale = scale_factor\n",
    "\n",
    "        self.norm_fine = nn.LayerNorm(d_model)\n",
    "        self.attn_fine = MultiHeadSelfAttention(d_model, num_heads, dropout)\n",
    "\n",
    "        self.norm_coarse = nn.LayerNorm(d_model)\n",
    "        self.attn_coarse = MultiHeadSelfAttention(d_model, num_heads, dropout)\n",
    "\n",
    "        self.fuse = nn.Linear(2 * d_model, d_model)\n",
    "\n",
    "        hidden = d_model * mlp_ratio\n",
    "        self.norm_ff = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape\n",
    "\n",
    "        # Escala fina\n",
    "        xf = self.norm_fine(x)\n",
    "        yf = self.attn_fine(xf)  # [B, T, D]\n",
    "\n",
    "        # Escala gruesa\n",
    "        xc = self.norm_coarse(x).transpose(1, 2)  # [B, D, T]\n",
    "        s = self.scale\n",
    "        cut = T - (T % s)\n",
    "        if cut == 0:\n",
    "            yc_up = torch.zeros_like(yf)\n",
    "        else:\n",
    "            xc = xc[:, :, :cut]                        # [B, D, cut]\n",
    "            pooled = F.avg_pool1d(xc, kernel_size=s, stride=s)  # [B, D, cut/s]\n",
    "            pooled = pooled.transpose(1, 2)           # [B, Lc, D]\n",
    "            yc = self.attn_coarse(pooled)             # [B, Lc, D]\n",
    "            yc_up = yc.repeat_interleave(s, dim=1)    # [B, cut, D]\n",
    "            if cut < T:\n",
    "                pad = yc_up[:, -1:, :].expand(B, T - cut, D)\n",
    "                yc_up = torch.cat([yc_up, pad], dim=1)\n",
    "\n",
    "        # Fusión y residuales\n",
    "        y = torch.cat([yf, yc_up], dim=-1)  # [B, T, 2D]\n",
    "        y = self.fuse(y)                    # [B, T, D]\n",
    "        x = x + y                           # residual 1\n",
    "\n",
    "        z = self.norm_ff(x)\n",
    "        z = self.ff(z)\n",
    "        x = x + z                           # residual 2\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cec61bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== 6) Modelo completo ====\n",
    "class NILMTransformerMultiScale(nn.Module):\n",
    "    \"\"\"\n",
    "    - Embedding lineal de entrada (W -> d_model)\n",
    "    - Positional encoding\n",
    "    - Varios bloques multiescala\n",
    "    - Cabeza lineal para regresión de potencia (W)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=128, num_heads=4, depth=4, scale_factor=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.in_proj  = nn.Linear(1, d_model)\n",
    "        self.pos_enc  = PositionalEncoding(d_model)\n",
    "        self.blocks   = nn.ModuleList([\n",
    "            MultiScaleBlock(d_model, num_heads, mlp_ratio=4, dropout=dropout, scale_factor=scale_factor)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm_out = nn.LayerNorm(d_model)\n",
    "        self.head     = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, T, 1]  (potencia agregada)\n",
    "        return: [B, T, 1] (potencia del aparato)\n",
    "        \"\"\"\n",
    "        x = self.in_proj(x)\n",
    "        x = self.pos_enc(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm_out(x)\n",
    "        y = self.head(x)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555c4686",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== 7) Dataset sintético ====\n",
    "class DummyDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Genera pares (x_agregada, y_aparato) artificiales para probar el modelo.\n",
    "    - y: pulsos aleatorios (aparato)\n",
    "    - x: y + tendencia + ruido + otros \"aparatos\"\n",
    "    \"\"\"\n",
    "    def __init__(self, T=512, N=1024, seed=0):\n",
    "        super().__init__()\n",
    "        g = torch.Generator().manual_seed(seed)\n",
    "        self.X, self.Y = [], []\n",
    "        for _ in range(N):\n",
    "            y = torch.zeros(T)\n",
    "            for _ in range(torch.randint(1, 5, (1,), generator=g)):\n",
    "                amp = torch.randint(500, 1500, (1,), generator=g).float()\n",
    "                start = torch.randint(0, T-20, (1,), generator=g).item()\n",
    "                width = torch.randint(5, 30, (1,), generator=g).item()\n",
    "                y[start:start+width] += amp\n",
    "            trend = torch.linspace(0, 200, T)\n",
    "            noise = torch.randn(T) * 50\n",
    "            others = F.relu(torch.sin(torch.linspace(0, 20, T))*200)\n",
    "            x = y + trend + noise + others\n",
    "            self.X.append(x.unsqueeze(-1))\n",
    "            self.Y.append(y.unsqueeze(-1))\n",
    "        self.X = torch.stack(self.X)  # [N, T, 1]\n",
    "        self.Y = torch.stack(self.Y)  # [N, T, 1]\n",
    "\n",
    "    def __len__(self): return self.X.size(0)\n",
    "    def __getitem__(self, i): return self.X[i], self.Y[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0adf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== 8) Entrenamiento mínimo ====\n",
    "def train_demo(epochs=5, batch_size=16, T=512, d_model=128, depth=4, scale_factor=4, lr=1e-3):\n",
    "    ds_train = DummyDataset(T=T, N=512, seed=0)\n",
    "    ds_val   = DummyDataset(T=T, N=128, seed=1)\n",
    "    train_loader = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = torch.utils.data.DataLoader(ds_val,   batch_size=batch_size)\n",
    "\n",
    "    model = NILMTransformerMultiScale(d_model=d_model, num_heads=4, depth=depth,\n",
    "                                      scale_factor=scale_factor, dropout=0.1).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.L1Loss()\n",
    "\n",
    "    def evaluate():\n",
    "        model.eval()\n",
    "        total = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                pred = model(xb)\n",
    "                loss = loss_fn(pred, yb)\n",
    "                total += loss.item() * xb.size(0)\n",
    "        return total / len(ds_val)\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            opt.step()\n",
    "        val_mae = evaluate()\n",
    "        print(f\"Epoch {ep:02d} | Val MAE: {val_mae:.2f}\")\n",
    "    return model\n",
    "\n",
    "model = train_demo(epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5967e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== 9) Visualización de predicciones ====\n",
    "ds_vis = DummyDataset(T=512, N=4, seed=2)\n",
    "xv, yv = ds_vis[0]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pv = model(xv.unsqueeze(0).to(device)).cpu().squeeze(0).squeeze(-1)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(xv.squeeze(-1).numpy(), label=\"Agregada (entrada)\")\n",
    "plt.plot(yv.squeeze(-1).numpy(), label=\"Aparato (verdad)\", linewidth=2)\n",
    "plt.plot(pv.numpy(), label=\"Predicción (modelo)\", linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.title(\"NILM — Predicción de aparato (demo sintética)\")\n",
    "plt.xlabel(\"Tiempo (pasos)\")\n",
    "plt.ylabel(\"Potencia [W]\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e2db9a",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Cómo conectarlo a un dataset real (UK-DALE / REFIT)\n",
    "\n",
    "1. **Carga de datos**: convierte la señal agregada y la del aparato objetivo a un `numpy.ndarray` o `torch.Tensor` de forma \\[T, 1], re-muestreado a **1 Hz** (o 6 s) de forma consistente.\n",
    "2. **Ventaneo**: corta en ventanas de longitud `T` (p. ej. 3600) con solape (50%). Construye un `Dataset` como `DummyDataset` pero leyendo de tus arrays.\n",
    "3. **Normalización**: escala potencias (p. ej., divide por 1000 para kW), y guarda los factores para des-escala en la salida.\n",
    "4. **Entrenamiento**: crea un `DataLoader` y llama a `train_demo(...)` adaptado (pasando tu dataset).\n",
    "5. **Métricas**: añade cálculo de MAE, RMSE, SAE/NDE y F1 (umbralizando On/Off). Puedes crear una celda adicional para métricas por aparato.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
